{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9b269c-704f-4c87-bbd6-91b225bfb001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# File paths\n",
    "document_folder = \"./data\"  # Folder containing text documents\n",
    "stopword_file = './stopwords.txt'  # Stopword list\n",
    "\n",
    "# Load stopwords\n",
    "with open(stopword_file, 'r') as f:\n",
    "    stop_words = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "def tokenization(text):\n",
    "    # 使用正則表達式一次性移除標點和數字\n",
    "    doc = re.sub(r\"[^\\w\\s]|[\\d]\", \" \", text)  # 移除標點符號和數字\n",
    "    \n",
    "    # 將文本轉為小寫\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Tokenization: 使用 split() 切分並清理多餘空白\n",
    "    words = [word.strip() for word in doc.split() if word.strip()]\n",
    "    \n",
    "    # Porter's stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemming = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Stop words removal\n",
    "    token_list = [word for word in stemming if word not in stop_words]\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "# Load and preprocess documents\n",
    "documents = []\n",
    "for i in range(1, 1096):  # Document IDs range from 1 to 1095\n",
    "    file_path = os.path.join(document_folder, f\"{i}.txt\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        processed_content = tokenization(content)\n",
    "        documents.append(processed_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7133ba28-fa81-4127-b8c0-c7370551718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest term index is: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute document frequency (DF) for terms\n",
    "term_df = defaultdict(int)\n",
    "for doc in documents:\n",
    "    unique_terms = set(doc)\n",
    "    for term in unique_terms:\n",
    "        term_df[term] += 1\n",
    "\n",
    "# Sort terms alphabetically and assign indices\n",
    "sorted_terms = sorted(term_df.items(), key=lambda x: x[0])\n",
    "dictionary_entries = [(idx + 1, term, df) for idx, (term, df) in enumerate(sorted_terms)]\n",
    "\n",
    "# Compute IDF values\n",
    "N = 1095  # Total number of documents\n",
    "idf_dict = {}\n",
    "for idx, term, df in dictionary_entries:\n",
    "    idf_value = math.log10(N / df) if df > 0 else 0\n",
    "    idf_dict[term] = (idx, idf_value)\n",
    "\n",
    "# Compute normalized TF-IDF vectors\n",
    "sparse_tfidf_vectors = []\n",
    "for doc in documents:\n",
    "    term_count = defaultdict(int)\n",
    "    for term in doc:\n",
    "        term_count[term] += 1\n",
    "\n",
    "    total_terms = len(doc)\n",
    "    tfidf_vector = {}\n",
    "    for term, count in term_count.items():\n",
    "        tf = count / total_terms\n",
    "        idx, idf_value = idf_dict.get(term, (0, 0))\n",
    "        tfidf_vector[idx] = tf * idf_value\n",
    "\n",
    "    # Normalize TF-IDF vector\n",
    "    magnitude = np.linalg.norm(list(tfidf_vector.values()))\n",
    "    if magnitude > 0:\n",
    "        tfidf_vector = {idx: value / magnitude for idx, value in tfidf_vector.items()}\n",
    "\n",
    "    sparse_tfidf_vectors.append(tfidf_vector)\n",
    "\n",
    "# Convert sparse vectors to a dense matrix\n",
    "all_indices = sorted(idf_dict.values(), key=lambda x: x[0])\n",
    "tfidf_matrix = np.zeros((len(documents), len(all_indices)))\n",
    "for doc_idx, tfidf_vector in enumerate(sparse_tfidf_vectors):\n",
    "    for term_idx, value in tfidf_vector.items():\n",
    "\n",
    "        tfidf_matrix[doc_idx, term_idx - 1] = value\n",
    "        \n",
    "min_term_idx = float('inf')  # 設置一個很大的初始值\n",
    "for doc_idx, tfidf_vector in enumerate(sparse_tfidf_vectors):\n",
    "    for term_idx, value in tfidf_vector.items():\n",
    "        if term_idx < min_term_idx:\n",
    "            min_term_idx = term_idx\n",
    "\n",
    "print(f\"The smallest term index is: {min_term_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afaa787-3e2e-452f-bfbe-8b7b9f19dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxHeap:\n",
    "    def __init__(self):\n",
    "        self.heap = []\n",
    "\n",
    "    def _parent(self, index):\n",
    "        return (index - 1) // 2\n",
    "\n",
    "    def _left_child(self, index):\n",
    "        return 2 * index + 1\n",
    "\n",
    "    def _right_child(self, index):\n",
    "        return 2 * index + 2\n",
    "\n",
    "    def _heapify_up(self, index):\n",
    "        while index > 0 and self.heap[self._parent(index)][0] < self.heap[index][0]:\n",
    "            self.heap[self._parent(index)], self.heap[index] = self.heap[index], self.heap[self._parent(index)]\n",
    "            index = self._parent(index)\n",
    "\n",
    "    def _heapify_down(self, index):\n",
    "        largest = index\n",
    "        left = self._left_child(index)\n",
    "        right = self._right_child(index)\n",
    "\n",
    "        if left < len(self.heap) and self.heap[left][0] > self.heap[largest][0]:\n",
    "            largest = left\n",
    "        if right < len(self.heap) and self.heap[right][0] > self.heap[largest][0]:\n",
    "            largest = right\n",
    "        if largest != index:\n",
    "            self.heap[index], self.heap[largest] = self.heap[largest], self.heap[index]\n",
    "            self._heapify_down(largest)\n",
    "\n",
    "    def push(self, similarity, cluster1, cluster2):\n",
    "        self.heap.append((similarity, cluster1, cluster2))\n",
    "        self._heapify_up(len(self.heap) - 1)\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.heap) == 0:\n",
    "            return None\n",
    "        if len(self.heap) == 1:\n",
    "            return self.heap.pop()\n",
    "        max_value = self.heap[0]\n",
    "        self.heap[0] = self.heap.pop()\n",
    "        self._heapify_down(0)\n",
    "        return max_value\n",
    "\n",
    "    def peek(self):\n",
    "        return self.heap[0] if self.heap else None\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.heap)\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.heap) == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1b0f23-855d-4f18-acad-f3ec872ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2032423017240074\n"
     ]
    }
   ],
   "source": [
    "def compute_cosine_similarity(matrix):\n",
    "    N = matrix.shape[0]\n",
    "    similarity_matrix = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                similarity_matrix[i, j] = 1.0  # Cosine similarity with itself\n",
    "            else:\n",
    "                dot_product = np.dot(matrix[i], matrix[j])\n",
    "                norm_i = np.linalg.norm(matrix[i])\n",
    "                norm_j = np.linalg.norm(matrix[j])\n",
    "                if norm_i > 0 and norm_j > 0:\n",
    "                    similarity_matrix[i, j] = dot_product / (norm_i * norm_j)\n",
    "                else:\n",
    "                    similarity_matrix[i, j] = 0.0\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "similarity_matrix = compute_cosine_similarity(tfidf_matrix)\n",
    "print(similarity_matrix[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b7fd2a-4215-4063-a18d-1b2b0a07a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hierarchical_clustering(tfidf_matrix, similarity_matrix, K, method='complete-link'):\n",
    "    N = tfidf_matrix.shape[0]\n",
    "    print(N)\n",
    "\n",
    "    # Initialize clusters\n",
    "    clusters = {i: [i] for i in range(N)}\n",
    "\n",
    "    # Initialize custom heap\n",
    "    max_heap = MaxHeap()\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            max_heap.push(similarity, i, j)\n",
    "\n",
    "    while len(clusters) > K:\n",
    "        # Extract the most similar clusters\n",
    "        s, cluster1, cluster2 = max_heap.pop()\n",
    "        #print(s)\n",
    "\n",
    "        # Ensure clusters are still valid\n",
    "        if cluster1 in clusters and cluster2 in clusters:\n",
    "            # Merge clusters\n",
    "            clusters[cluster1].extend(clusters[cluster2])\n",
    "            del clusters[cluster2]\n",
    "            #print(f\"Current content of cluster1 (after merge): {clusters[cluster1]}\")\n",
    "\n",
    "            # Recalculate similarity with other clusters\n",
    "            for other_cluster in list(clusters.keys()):\n",
    "                if other_cluster != cluster1:\n",
    "                    if method == 'single-link':\n",
    "                        sim = max(\n",
    "                            similarity_matrix[i, j]\n",
    "                            for i in clusters[cluster1]\n",
    "                            for j in clusters[other_cluster]\n",
    "                        )\n",
    "                    elif method == 'complete-link':\n",
    "                        sim = min(\n",
    "                            similarity_matrix[i, j]\n",
    "                            for i in clusters[cluster1]\n",
    "                            for j in clusters[other_cluster]\n",
    "                        )\n",
    "                    max_heap.push(sim, cluster1, other_cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d84dea0-0f64-4185-9846-767df3b0ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "1095\n",
      "1095\n"
     ]
    }
   ],
   "source": [
    "def save_clusters(clusters, file_name):\n",
    "    # Sort the clusters by the minimum document ID in each cluster\n",
    "    sorted_clusters = {k: sorted(v) for k, v in clusters.items()}\n",
    "    sorted_clusters = dict(sorted(sorted_clusters.items(), key=lambda x: min(x[1])))\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        for cluster_id, doc_ids in sorted_clusters.items():\n",
    "            # Write doc_ids in a straight line, one per line\n",
    "            for doc_id in doc_ids:\n",
    "                f.write(f\"{doc_id+1}\\n\")\n",
    "            # Add an empty line after each cluster\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Perform clustering for K=8, 13, 20\n",
    "for K in [8, 13, 20]:\n",
    "    clusters = hierarchical_clustering(tfidf_matrix, similarity_matrix, K)\n",
    "    save_clusters(clusters, f\"{K}.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76753e0-fbee-46a2-9be7-dc016568a2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938be99-1def-4910-910a-256112fe8f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
